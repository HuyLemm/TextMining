{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total Examples in Train': 12000,\n",
       " 'First Example': {'context': 'Về mặt kiến \\u200b\\u200btrúc, trường có một nhân vật Công giáo. Trên đỉnh mái vòm vàng tòa nhà của chính là bức tượng vàng của Đức Trinh Nữ Maria. Ngay ở phía trước của Tòa nhà Chính và phải đối mặt với nó, là một bức tượng đồng của Chúa Kitô với vũ khí upraised với truyền thuyết \"Venite rao nhớ Omnes\". Bên cạnh Tòa nhà Chính là Vương Cung Thánh Đường Thánh Tâm Chúa. Ngay phía sau nhà thờ là Grotto, một nơi Marian cầu nguyện và suy. Nó là một bản sao của hang động ở Lourdes, Pháp nơi Đức Trinh Nữ Maria hiện ra với reputedly Saint Bernadette Soubirous trong năm 1858. Vào cuối của ổ đĩa chính (và trong một đường thẳng nối thông qua 3 tượng và Dome Gold), là một bức tượng đá hiện đại đơn giản của Mary. .',\n",
       "  'qas': [{'id': '5733be284776f4190066117e',\n",
       "    'question': 'Có gì ngồi trên đầu trang của Tòa nhà Chính tại Notre Dame?',\n",
       "    'answers': [{'text': 'bức tượng vàng của Đức Trinh Nữ Maria',\n",
       "      'answer_start': 98}],\n",
       "    'is_impossible': False},\n",
       "   {'id': '5733be284776f4190066117f',\n",
       "    'question': 'là những gì ở phía trước của Notre Dame Tòa nhà Chính?',\n",
       "    'answers': [{'text': 'một bức tượng đồng của Chúa Kitô',\n",
       "      'answer_start': 200}],\n",
       "    'is_impossible': False},\n",
       "   {'id': '5733be284776f41900661180',\n",
       "    'question': 'Vương Cung Thánh Đường của trái tim Thánh tại Notre Dame là bên cạnh để mà cấu trúc?',\n",
       "    'answers': [{'text': 'Tòa nhà Chính', 'answer_start': 304}],\n",
       "    'is_impossible': False},\n",
       "   {'id': '5733be284776f41900661181',\n",
       "    'question': 'các Grotto tại Notre Dame là gì?',\n",
       "    'answers': [{'text': 'một nơi Marian cầu nguyện và suy',\n",
       "      'answer_start': 393}],\n",
       "    'is_impossible': False},\n",
       "   {'id': '5733be284776f41900661182',\n",
       "    'question': 'Mà đã bị cáo buộc Đức Trinh Nữ Maria xuất hiện năm 1858 tại Lourdes Pháp?',\n",
       "    'answers': [{'text': 'Saint Bernadette Soubirous', 'answer_start': 520}],\n",
       "    'is_impossible': False}]},\n",
       " 'Context Length': 700,\n",
       " 'Total Questions in First Example': 5,\n",
       " 'First Question': {'id': '5733be284776f4190066117e',\n",
       "  'question': 'Có gì ngồi trên đầu trang của Tòa nhà Chính tại Notre Dame?',\n",
       "  'answers': [{'text': 'bức tượng vàng của Đức Trinh Nữ Maria',\n",
       "    'answer_start': 98}],\n",
       "  'is_impossible': False}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Đường dẫn đến các file dữ liệu\n",
    "train_file_path = 'Project2_Data/train.json'\n",
    "test_file_path = 'Project2_Data/test.json'\n",
    "\n",
    "# Hàm để đọc dữ liệu từ file JSON\n",
    "def read_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Đọc dữ liệu từ file\n",
    "train_data = read_json(train_file_path)\n",
    "test_data = read_json(test_file_path)\n",
    "\n",
    "# Hiển thị cấu trúc của dữ liệu\n",
    "train_data_structure = {\n",
    "    \"Total Examples in Train\": len(train_data),\n",
    "    \"First Example\": train_data[0],\n",
    "    \"Context Length\": len(train_data[0]['context']),\n",
    "    \"Total Questions in First Example\": len(train_data[0]['qas']),\n",
    "    \"First Question\": train_data[0]['qas'][0]\n",
    "}\n",
    "\n",
    "test_data_structure = {\n",
    "    \"Total Examples in Test\": len(test_data),\n",
    "    \"First Example\": test_data[0],\n",
    "    \"Context Length\": len(test_data[0]['context']),\n",
    "    \"Total Questions in First Example\": len(test_data[0]['qas']),\n",
    "    \"First Question\": test_data[0]['qas'][0]\n",
    "}\n",
    "\n",
    "train_data_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Huy Anh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sử dụng XLNetTokenizerFast\n",
    "tokenizer = XLNetTokenizerFast.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "def encode_data(data):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    is_impossible = []\n",
    "\n",
    "    for item in data:\n",
    "        context = item['context']\n",
    "        for qa in item['qas']:\n",
    "            question = qa['question']\n",
    "            # Encode câu hỏi và context\n",
    "            encoded = tokenizer.encode_plus(question, context, max_length=512, truncation=True, padding='max_length', return_tensors='pt', return_offsets_mapping=True)\n",
    "            input_id = encoded['input_ids']\n",
    "            attention_mask = encoded['attention_mask']\n",
    "            offset_mapping = encoded['offset_mapping'][0].tolist()\n",
    "\n",
    "            # Mặc định giá trị cho start và end position\n",
    "            start_pos = 0\n",
    "            end_pos = 0\n",
    "\n",
    "            # Xử lý câu trả lời\n",
    "            if not qa['is_impossible']:\n",
    "                answer = qa['answers'][0]['text']\n",
    "                start_char = qa['answers'][0]['answer_start']\n",
    "                end_char = start_char + len(answer)\n",
    "\n",
    "                # Tìm vị trí token của câu trả lời\n",
    "                sequence_ids = encoded.sequence_ids()\n",
    "                for i, (offset_start, offset_end) in enumerate(offset_mapping):\n",
    "                    if sequence_ids[i] == 1:  # Phần của context\n",
    "                        if start_char >= offset_start and end_char <= offset_end:\n",
    "                            start_pos = i\n",
    "                            end_pos = i\n",
    "                            break\n",
    "                        elif start_char < offset_end and end_pos == 0:\n",
    "                            start_pos = i\n",
    "                        if end_char > offset_start and start_pos != 0:\n",
    "                            end_pos = i\n",
    "\n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "            start_positions.append(start_pos)\n",
    "            end_positions.append(end_pos)\n",
    "            is_impossible.append(int(qa['is_impossible']))\n",
    "    \n",
    "    # Chuyển đổi lists thành tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    start_positions = torch.tensor(start_positions, dtype=torch.long)\n",
    "    end_positions = torch.tensor(end_positions, dtype=torch.long)\n",
    "    is_impossible = torch.tensor(is_impossible, dtype=torch.long)  # Thay đổi dtype sang torch.long\n",
    "    \n",
    "    return input_ids, attention_masks, start_positions, end_positions, is_impossible\n",
    "\n",
    "# Tiền xử lý dữ liệu\n",
    "# train_data và test_data đã được định nghĩa từ bước trước\n",
    "train_input_ids, train_attention_masks, train_start_positions, train_end_positions, train_is_impossible = encode_data(train_data)\n",
    "test_input_ids, test_attention_masks, test_start_positions, test_end_positions, test_is_impossible = encode_data(test_data)\n",
    "\n",
    "# Tạo TensorDataset và DataLoader\n",
    "batch_size = 6\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_start_positions, train_end_positions, train_is_impossible)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_start_positions, test_end_positions, test_is_impossible)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Input IDs: tensor([  574,  2582,  2483,    17,   267,  2762,  2721,  5835, 10479,   150,\n",
      "           17,     0,    93,    17,  2970, 23444,  9712,    17,  1598,   430,\n",
      "        17314, 13781,   369,    82,     4,    17,     0,   155,  2483,    17,\n",
      "         7575,    17,  6684,   150, 10479,   150,    17,  3192,    17, 13836,\n",
      "           98,   102,   627,    17,  3525,   101,    17,     0,    93,    17,\n",
      "         2970, 23444,  9712,    17,  1598,   430, 17314, 13781,   369,    17,\n",
      "         2853, 19865,    13, 26052,    17,  1598,    17,    23,  2168,    17,\n",
      "            0,   155, 10479,   150,    17,  3192,    17, 13836,    98,   102,\n",
      "          627,    17,  3525,   101,   128,  7735, 18551,    23,    17,    46,\n",
      "        16983,   425,   963,     9,   330,  2996,  2003,  2874,    66,  4931,\n",
      "           19,   541,   815,    13, 26214,    19,    17,  4063,  5835,  6938,\n",
      "          150,    17, 13836,    98,   102,   627,  3074,  2505,    17,  3525,\n",
      "          101,    17,   267,  2762,  2721,  5835,    17,  1598,  6353,    17,\n",
      "        16983,  5460,   369,   100,   409,    17, 11276,  2184,  5022,   443,\n",
      "           17,  1598,   322,  2390,  9044,    17,    10,  1346,    17,  8583,\n",
      "          100,   409,    17, 11276,    17, 17164,   117,    17, 17007,  2483,\n",
      "           17, 12387,    17, 11768,    17,  3525,   101, 16446,    11,    17,\n",
      "          661,    17,     0,   101,    17,  2530,   100,   409,    17,    46,\n",
      "        17314,    17,  7575,  3887,  6938,   101,    17,  3525,   101,  1241,\n",
      "           17,  1598,    17,   267,  2762,  2721,  5835,    17,     0,   660,\n",
      "         3374,    17, 10698,    46,    17,     0,   254,    17,   180,  2762,\n",
      "        15646,  8304,   262,     9,   330,  2996,  2003,  2874,    66,  4931,\n",
      "           19,   541,   815,    13, 26214,    19,    17,  4063,  5835,  6938,\n",
      "          150,    17, 13836,    98,   102,   627,  3074,  2505,    17,  3525,\n",
      "          101,    17,   267,  2762,  2721,  5835,    17,  1598,  6353,    17,\n",
      "        16983,  5460,   369,   100,   409,    17, 11276,  2184,  5022,   443,\n",
      "           17,  1598,   322,  2390,  9044,    17,    10,  1346,    17,  8583,\n",
      "          100,   409,    17, 11276,    17, 17164,   117,    17, 17007,  2483,\n",
      "           17, 12387,    17, 11768,    17,  3525,   101, 16446,    11,    17,\n",
      "          661,    17,     0,   101,    17,  2530,   100,   409,    17,    46,\n",
      "        17314,    17,  7575,  3887,  6938,   101,    17,  3525,   101,  1241,\n",
      "           17,  1598,    17,   267,  2762,  2721,  5835,    17,     0,   660,\n",
      "         3374,    17, 10698,    46,    17,     0,   254,    17,   180,  2762,\n",
      "        15646,  8304,   262,     9,   330,  2996,  2003,  2874,    66,  4931,\n",
      "           19,   541,   815,    13, 26214,    19,    17,  4063,  5835,  6938,\n",
      "          150,    17, 13836,    98,   102,   627,  3074,  2505,    17,  3525,\n",
      "          101,    17,   267,  2762,  2721,  5835,    17,  1598,  6353,    17,\n",
      "        16983,  5460,   369,   100,   409,    17, 11276,  2184,  5022,   443,\n",
      "           17,  1598,   322,  2390,  9044,    17,    10,  1346,    17,  8583,\n",
      "          100,   409,    17, 11276,    17, 17164,   117,    17, 17007,  2483,\n",
      "           17, 12387,    17, 11768,    17,  3525,   101, 16446,    11,    17,\n",
      "          661,    17,     0,   101,    17,  2530,   100,   409,    17,    46,\n",
      "        17314,    17,  7575,  3887,  6938,   101,    17,  3525,   101,  1241,\n",
      "           17,  1598,    17,   267,  2762,  2721,  5835,    17,     0,   660,\n",
      "         3374,    17, 10698,    46,    17,     0,   254,    17,   180,  2762,\n",
      "        15646,  8304,   262,     9,   330,  2996,  2003,  2874,    66,  4931,\n",
      "           19,   541,   815,    13, 26214,    19,    17,  4063,  5835,  6938,\n",
      "          150,    17, 13836,    98,   102,   627,  3074,  2505,    17,  3525,\n",
      "          101,    17,   267,  2762,  2721,  5835,    17,  1598,  6353,    17,\n",
      "        16983,  5460,   369,   100,   409,    17, 11276,  2184,  5022,   443,\n",
      "           17,  1598,   322,  2390,  9044,    17,    10,  1346,    17,  8583,\n",
      "            4,     3])\n",
      "Attention Masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Start Position: tensor(61)\n",
      "End Position: tensor(63)\n",
      "Is Impossible: tensor(0)\n",
      "\n",
      "\n",
      "Sample 2:\n",
      "Input IDs: tensor([    5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
      "            5,     5,     5,     5,  9892,   409,    17,     0,   254,    17,\n",
      "         8353,  1516,    19,    17,  6798,    17,    46,   627,  5460,  5460,\n",
      "          369,    17,   180,  1664,    17,  6684, 14638, 28820,    17,  2416,\n",
      "          101,    17,  1022,    17,   180, 20841,    17,  3141,    82,     4,\n",
      "         4036,   254,    17, 17007,    19,    17,    10, 12030,    11,    19,\n",
      "          830,    17,  3511, 17024,   254,    17,    46,  2996,    17,  3525,\n",
      "          101,   943, 18379,    17,  2311,   627, 18934,    17,   787,  2517,\n",
      "           17,  3744,  5460,   369,    17,   180,  1664,    17,  6684,    17,\n",
      "           46, 16983, 14638, 28820,    97,    17,  3192,    17,   577,    17,\n",
      "        13310,    17,  3525,   101,  6938,   830,    18,    17,     0,   660,\n",
      "         3374,    17,  2686,   150,    17,   138,  5835,    17,   775,   100,\n",
      "          409,  5977,    97,  6353,   150, 13275,   409,  6938,  2996,    17,\n",
      "          180,  1664,    17,  6684,    17,     0,  2555,   216,   450,    17,\n",
      "          138,  2555,    17,  4376,    43,    17,   138,  2555,   943, 18379,\n",
      "           17,  2311,   627,    17,  1204,  5618,    17,  3525,   101,    17,\n",
      "         1841,   409,    17,  2686,   150,  5460,   369,    17,  3511, 17024,\n",
      "          254,    17,    46,  2996,    17,  3525,   101,    17, 12519,   155,\n",
      "           17,  2311,   627,    17, 11276,  5854,   254,    17,    46, 16983,\n",
      "         1387,   299,    17,     0,   155,   262,    97,    17,   180, 20841,\n",
      "           17,  3511, 17024,   254,    17,    46,  2996,    17, 17007,   830,\n",
      "           17, 12387,   830,    17,  4376,   180,   830,    17,    23,  5835,\n",
      "           17,    46,  2762, 21247,    17,  4063,  5835,   326,   409,   110,\n",
      "           17,  1598,  2483,    17, 14863,    17, 13346,    17,  3744,    17,\n",
      "         3192,    17,  2853,    17,   180,  2643,    17,   138,  5835,    17,\n",
      "         3525,   101,  6353,   150,    17,   180,  3017, 13114,    17,    46,\n",
      "        16983,   102,  5460,   369,    17,   180,  1664,    17,  6684,    17,\n",
      "         3525,   101,  1387,   299,    17,     0,   155,   262,    97,    17,\n",
      "          180, 20841,    17,  3511, 17024,   254,    17,    46,  2996,    17,\n",
      "        17007,   830,    17, 12387,  1160,    17, 19908,    17,  2311,  2555,\n",
      "           19,    17,  1598,    17,  6026,   101,    17,    46, 16983,    17,\n",
      "         1841,   409,    17, 10656,    17,   180,  2075,    17,  8024,    46,\n",
      "         8265,   254,    17,  5734,    17,  4216,   101,    97,    17,  1598,\n",
      "           17,  3192,    17,   577,    17, 13310,    17,  3525,   101,    17,\n",
      "          180, 20841,    17,  3511, 17024,   254,    17,    46,  2996,  5460,\n",
      "           17,  8583,    17,   787,  2517,    22,    17,   138,  2555,    17,\n",
      "          180,  9814,    17,  1598,    17,   138,  5835,    17,     0,  4013,\n",
      "           17,     0,   155,   262,    17, 12340,    17,    46, 16983,  1387,\n",
      "          299,    17,     0,  2555,   460,   409,    17, 12519,   155,     9,\n",
      "            4,     3])\n",
      "Attention Masks: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Start Position: tensor(181)\n",
      "End Position: tensor(196)\n",
      "Is Impossible: tensor(0)\n",
      "\n",
      "\n",
      "Sample 3:\n",
      "Input IDs: tensor([   17,   180,  1664,    17,  3192,  6938,   369,    17,   180,  3017,\n",
      "        13114,   395, 18379,    17,    46,  2762,  2555,    17,  3192,    17,\n",
      "           66,  2517,    17,     0,  2168,    17,   661,  8265,  4269,    17,\n",
      "            0,  5936,  1261,    17,  6838,  6938,    17,  2686,   150,    17,\n",
      "         3192,  6842,    17,     0,  2168,    17,  3525,   101,  3619,  2996,\n",
      "          578,  1187,    82,     4,   659,    17,  5235,    36,    17,  3192,\n",
      "           17,     0,  2555,    17,   138, 13403,    17,   189,    17,  3192,\n",
      "           17,  6049, 13114,    17,     0,  2168, 10170,   409,  3512,  2996,\n",
      "           17,  3525,   101,  6874,    20,  2073,  3923,   254,   889,    97,\n",
      "           17,     0,  2168,    18,    17,  3531,   501,    17,    10,  1531,\n",
      "         3699,    11,  6938,  2996,    17,  3141,  6447,    18,    17,  3531,\n",
      "          483,    17,    10,  1608,  1464,    11,    17,    46,  2762,  2555,\n",
      "           17,     0,   660,  3374,    17,  3192,    17,    66,  2517,    17,\n",
      "          180,  2762,  5460,   369,    17,  6684,   150,    17,  3531,     9,\n",
      "           17,   180,  1664,    17,  3192,  6938,   369,    17,   180,  3017,\n",
      "        13114,   395, 18379,    17,    46,  2762,  2555,    17,     0,   627,\n",
      "           17,  6684,   150,    17, 18976,    19,    17,     0,   660,  3374,\n",
      "          216,   150,  2483, 19173,  6449,  9127, 21668,  2151,   202,    17,\n",
      "           10,   305,    17,  3531,    17,  3525,   101,    17, 12519,   369,\n",
      "           17,   180,  1187,    11,    19,    17,  3141,  6447,   483,  1522,\n",
      "           17,  1598,   483,  4406,    19,    17,  2853,    17,     0,  2168,\n",
      "         8265,  4269,    17,     0,  5936,    17,  3525,   101,  2769,  1404,\n",
      "         1142,    17,  3744,    17,     0,   254,    17,   267,  2582,    17,\n",
      "         3525,  3374,   330,  3756,   326,   299,   395, 18379,     9, 13039,\n",
      "           17,     0,  6447,    17,  2530,    17, 12104,    17,  1598,   155,\n",
      "           17,  3141,  6447,    18,    17,  3531,   483,    19,    17,  6684,\n",
      "          150,    17,     0,  5936,   460,   409, 18418,    17,  1022,    18,\n",
      "          155,   112,   150,    17,   180,  3017,   253,   216,   369,    17,\n",
      "         3525,   101,   116,    17,     0,    93,  4229, 14448,    17,  3525,\n",
      "          101,  6654,  2098,  4247,    17,   189,   395,  2762,  2555,   943,\n",
      "        18379,    19,  3512,  6447,    46,  2440,    17,  8353,   504,  3618,\n",
      "            9,   448,   395, 18379,    19,    17,   180,  2582,  4269,    17,\n",
      "           46,  3927,    17,  4731,  5460,   369,    48,   943,  1714,    17,\n",
      "         3525,   101,  5489,   180,  6949,   780, 10300,  9687, 17845, 12415,\n",
      "        12874,    17,    46, 16983,   504,  4604,     9,   578,  3017, 13114,\n",
      "           17,   751,  5977,    17,  2311,   262,    17,  3525,   101,    17,\n",
      "          180,  2582,  4269,    17,   180,  1664,    17,  3192,  6938,   369,\n",
      "           17,  1598,  8265,   993,  6938,   369, 14991,    17,  3192,  6842,\n",
      "           17,     0,  2168,    17,  3525,   101,    17, 12519,   369,    17,\n",
      "          180,  1187,  2483,    17,   267,  2582,  6654,  2098,  4247,    17,\n",
      "         6026,   117,   254,   830,   102,    17,   180,  2643,    17,   138,\n",
      "         5835,    17,  2311,   262,    17,  2853,   830,    17,  4376,   180,\n",
      "           17,  7639,    17,   180,  1187,  5936,    17,     0,    93,    17,\n",
      "         2409,   369,    17,  5863,   116,   150,    17,  1238,    17,  1238,\n",
      "          150,    17,   118,  1187,  6917,    17, 13020, 10107,    17,  2555,\n",
      "         3512,  6447,    46,  2440,    17,  8353,   504,  3618,     9,   578,\n",
      "         2762,    17,     0,    93,    17, 12340,    17,   138,  5835,    17,\n",
      "         3525,   101,   116,    19,    17, 23158,    43,    46,  5460,   369,\n",
      "         6938,   369,    17, 12519,    17,  3192,    17,    66,  2517,    17,\n",
      "          180, 20841,    17,  8353,    17,  3525, 13114,  2349,  2517,    17,\n",
      "         3525,   101,    18,    17,  3531,    17, 17007,    17,    19,    17,\n",
      "            4,     3])\n",
      "Attention Masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Start Position: tensor(218)\n",
      "End Position: tensor(220)\n",
      "Is Impossible: tensor(0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In ra một số mẫu dữ liệu từ train dataset để kiểm tra\n",
    "def print_sample_data(input_ids, attention_masks, start_positions, end_positions, is_impossible, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(\"Input IDs:\", input_ids[i])\n",
    "        print(\"Attention Masks:\", attention_masks[i])\n",
    "        print(\"Start Position:\", start_positions[i])\n",
    "        print(\"End Position:\", end_positions[i])\n",
    "        print(\"Is Impossible:\", is_impossible[i])\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Chọn một số mẫu ngẫu nhiên để kiểm tra\n",
    "import random\n",
    "\n",
    "random_indices = random.sample(range(len(train_input_ids)), 3)\n",
    "sample_input_ids = train_input_ids[random_indices]\n",
    "sample_attention_masks = train_attention_masks[random_indices]\n",
    "sample_start_positions = train_start_positions[random_indices]\n",
    "sample_end_positions = train_end_positions[random_indices]\n",
    "sample_is_impossible = train_is_impossible[random_indices]\n",
    "\n",
    "print_sample_data(sample_input_ids, sample_attention_masks, sample_start_positions, sample_end_positions, sample_is_impossible)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForQuestionAnsweringSimple were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Chuẩn Bị Mô Hình\n",
    "# Khởi tạo tokenizer và mô hình\n",
    "\n",
    "from transformers import XLNetForQuestionAnsweringSimple, AdamW, XLNetTokenizerFast\n",
    "\n",
    "tokenizer = XLNetTokenizerFast.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetForQuestionAnsweringSimple.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Huy Anh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cấu Hình Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn Bị Chu kỳ Huấn Luyện\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  random_split\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% dữ liệu cho huấn luyện\n",
    "validation_size = len(train_dataset) - train_size  # 20% còn lại cho validation\n",
    "\n",
    "# Chia dataset\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_size, validation_size])\n",
    "\n",
    "# Tạo DataLoader cho phần train và validation\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "validation_dataloader = DataLoader(validation_dataset, shuffle=False, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "Updated best model parameters at batch 0 with loss 6.687341690063477\n",
      "Updated best model parameters at batch 1 with loss 6.386669158935547\n",
      "Updated best model parameters at batch 2 with loss 5.938324928283691\n",
      "Updated best model parameters at batch 5 with loss 5.717538833618164\n",
      "Updated best model parameters at batch 6 with loss 5.667553901672363\n",
      "Updated best model parameters at batch 10 with loss 5.592215538024902\n",
      "Updated best model parameters at batch 12 with loss 5.477365493774414\n",
      "Updated best model parameters at batch 13 with loss 5.025311470031738\n",
      "Updated best model parameters at batch 24 with loss 4.970441818237305\n",
      "Updated best model parameters at batch 25 with loss 4.935801029205322\n",
      "Updated best model parameters at batch 30 with loss 4.126447677612305\n",
      "Updated best model parameters at batch 56 with loss 4.048372745513916\n",
      "Updated best model parameters at batch 75 with loss 3.972717761993408\n",
      "Updated best model parameters at batch 94 with loss 3.9593560695648193\n",
      "Updated best model parameters at batch 107 with loss 3.862988233566284\n",
      "Updated best model parameters at batch 110 with loss 3.4653303623199463\n",
      "Updated best model parameters at batch 129 with loss 3.46500301361084\n",
      "Updated best model parameters at batch 152 with loss 3.464149236679077\n",
      "Updated best model parameters at batch 159 with loss 3.3556299209594727\n",
      "Updated best model parameters at batch 178 with loss 3.345782518386841\n",
      "Updated best model parameters at batch 197 with loss 3.21240234375\n",
      "  Batch 200 of 5005. Elapsed: 0:26:19.\n",
      "Updated best model parameters at batch 211 with loss 2.713595390319824\n",
      "Updated best model parameters at batch 242 with loss 2.4691500663757324\n",
      "Updated best model parameters at batch 320 with loss 1.8501617908477783\n",
      "Updated best model parameters at batch 350 with loss 1.7726268768310547\n",
      "  Batch 400 of 5005. Elapsed: 0:52:07.\n",
      "Updated best model parameters at batch 438 with loss 1.1762382984161377\n",
      "  Batch 600 of 5005. Elapsed: 1:17:53.\n",
      "  Batch 800 of 5005. Elapsed: 1:43:43.\n",
      "Updated best model parameters at batch 889 with loss 1.0221136808395386\n",
      "  Batch 1000 of 5005. Elapsed: 2:09:31.\n",
      "  Batch 1200 of 5005. Elapsed: 2:35:20.\n",
      "Updated best model parameters at batch 1306 with loss 0.7760379314422607\n",
      "  Batch 1400 of 5005. Elapsed: 3:01:07.\n",
      "  Batch 1600 of 5005. Elapsed: 3:26:53.\n",
      "  Batch 1800 of 5005. Elapsed: 3:52:39.\n",
      "  Batch 2000 of 5005. Elapsed: 4:18:25.\n",
      "  Batch 2200 of 5005. Elapsed: 4:44:11.\n",
      "  Batch 2400 of 5005. Elapsed: 5:09:58.\n",
      "  Batch 2600 of 5005. Elapsed: 5:35:45.\n",
      "  Batch 2800 of 5005. Elapsed: 6:01:32.\n",
      "  Batch 3000 of 5005. Elapsed: 6:27:20.\n",
      "Updated best model parameters at batch 3000 with loss 0.3935818672180176\n",
      "  Batch 3200 of 5005. Elapsed: 6:53:10.\n",
      "  Batch 3400 of 5005. Elapsed: 7:18:57.\n",
      "  Batch 3600 of 5005. Elapsed: 7:45:47.\n",
      "  Batch 3800 of 5005. Elapsed: 8:11:35.\n",
      "  Batch 4000 of 5005. Elapsed: 8:37:25.\n",
      "  Batch 4200 of 5005. Elapsed: 9:03:14.\n",
      "  Batch 4400 of 5005. Elapsed: 9:29:04.\n",
      "  Batch 4600 of 5005. Elapsed: 9:54:53.\n",
      "  Batch 4800 of 5005. Elapsed: 10:20:42.\n",
      "  Batch 5000 of 5005. Elapsed: 10:46:32.\n",
      "\n",
      "  Average training loss: 2.62\n",
      "  Training epoch took: 10:47:07\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch 200 of 5005. Elapsed: 0:25:50.\n",
      "  Batch 400 of 5005. Elapsed: 0:51:39.\n",
      "  Batch 600 of 5005. Elapsed: 1:17:28.\n",
      "  Batch 800 of 5005. Elapsed: 1:43:17.\n",
      "Updated best model parameters at batch 814 with loss 0.34005874395370483\n",
      "  Batch 1000 of 5005. Elapsed: 2:09:06.\n",
      "  Batch 1200 of 5005. Elapsed: 2:34:57.\n",
      "  Batch 1400 of 5005. Elapsed: 3:00:47.\n",
      "  Batch 1600 of 5005. Elapsed: 3:26:34.\n",
      "  Batch 1800 of 5005. Elapsed: 3:52:31.\n",
      "  Batch 2000 of 5005. Elapsed: 4:18:14.\n",
      "  Batch 2200 of 5005. Elapsed: 4:43:59.\n",
      "  Batch 2400 of 5005. Elapsed: 5:09:46.\n",
      "  Batch 2600 of 5005. Elapsed: 5:35:32.\n",
      "  Batch 2800 of 5005. Elapsed: 6:01:17.\n",
      "  Batch 3000 of 5005. Elapsed: 6:27:03.\n",
      "  Batch 3200 of 5005. Elapsed: 6:52:48.\n",
      "  Batch 3400 of 5005. Elapsed: 7:18:35.\n",
      "  Batch 3600 of 5005. Elapsed: 7:44:22.\n",
      "  Batch 3800 of 5005. Elapsed: 8:10:08.\n",
      "  Batch 4000 of 5005. Elapsed: 8:35:54.\n",
      "Updated best model parameters at batch 4103 with loss 0.27669060230255127\n",
      "  Batch 4200 of 5005. Elapsed: 9:01:41.\n",
      "  Batch 4400 of 5005. Elapsed: 9:27:29.\n",
      "  Batch 4600 of 5005. Elapsed: 9:53:16.\n",
      "  Batch 4800 of 5005. Elapsed: 10:19:03.\n",
      "  Batch 5000 of 5005. Elapsed: 10:44:49.\n",
      "\n",
      "  Average training loss: 1.94\n",
      "  Training epoch took: 10:45:23\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "# Thiết lập thiết bị\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Hàm để định dạng thời gian trôi qua\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "# Khởi tạo biến để lưu trọng số tốt nhất\n",
    "best_loss = float('inf')\n",
    "best_params_path = 'best_params.txt'  # Đường dẫn tệp để lưu thông số tốt nhất\n",
    "\n",
    "# Bắt đầu huấn luyện\n",
    "for epoch_i in range(epochs):\n",
    "    print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n",
    "    print(\"Training...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 200 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f\"  Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}.\")\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_start_positions, b_end_positions = batch[:4]\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, start_positions=b_start_positions, end_positions=b_end_positions)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Cập nhật và lưu thông số nếu tốt hơn\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            with open(best_params_path, 'w') as f:\n",
    "                for name, param in model.named_parameters():\n",
    "                    f.write(f\"{name}: {param.data}\\n\")\n",
    "            print(f\"Updated best model parameters at batch {step} with loss {best_loss}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(f\"\\n  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(f\"  Training epoch took: {training_time}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.61\n",
      "Average Test Loss: 1.54\n",
      "Testing took: 2:02:06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "# Chuyển mô hình sang chế độ đánh giá\n",
    "model.eval()\n",
    "\n",
    "# Theo dõi các biến để tính toán độ chính xác và loss\n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "\n",
    "# Ghi lại thời gian bắt đầu\n",
    "t0 = time.time()\n",
    "\n",
    "# Không cần tính toán hoặc lưu gradient\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_mask, b_start_positions, b_end_positions = batch[:4]\n",
    "\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_mask, start_positions=b_start_positions, end_positions=b_end_positions)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Lấy logit dự đoán cho vị trí bắt đầu và kết thúc\n",
    "        start_logits = outputs.start_logits.detach().cpu().numpy()\n",
    "        end_logits = outputs.end_logits.detach().cpu().numpy()\n",
    "        start_positions = b_start_positions.to('cpu').numpy()\n",
    "        end_positions = b_end_positions.to('cpu').numpy()\n",
    "\n",
    "        # Chuyển logit thành index dự đoán\n",
    "        start_preds = np.argmax(start_logits, axis=-1)\n",
    "        end_preds = np.argmax(end_logits, axis=-1)\n",
    "\n",
    "        # Tính độ chính xác cho batch\n",
    "        eval_accuracy = (np.sum(start_preds == start_positions) + np.sum(end_preds == end_positions)) / (len(start_preds) + len(end_preds))\n",
    "        total_eval_accuracy += eval_accuracy\n",
    "\n",
    "# Tính và in ra các thông số đánh giá\n",
    "avg_loss = total_eval_loss / len(test_dataloader)\n",
    "avg_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "total_time = format_time(time.time() - t0)  \n",
    "\n",
    "print(f\"Test Accuracy: {avg_accuracy:.2f}\")\n",
    "print(f\"Average Test Loss: {avg_loss:.2f}\")\n",
    "print(f\"Testing took: {total_time}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
